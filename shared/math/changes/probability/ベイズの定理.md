ベイズ確率の解釈では、定理は確率として表現された信念の度合いが、関連する証拠の入手可能性を考慮して合理的にどのように変化すべきかを表現している。

あるいは、計測されたデータ（結果）を下に、原因を推定する。

二つの事象 H (仮説 Hypothesis), D (情報 Data) があるとして、ベイズの定理は以下のように定義される。

$$
P(H|D) = \frac{P(H)L(H|D)}{P(D)} = \frac{P(H)P(D|H)}{P(D)}
$$

- 事後確率 $P(H|D)$: データ D を踏まえて、仮説 H が正しい確率
- 事前確率 $P(H)$: データ D がない状態で、仮説 H が正しい確率
- 尤度 $P(D|H) = L(H|D)$: 仮説 H が正しいとした場合に、データ D が得られる確率
- 周辺尤度 $P(D)$: 仮説 H の真偽を問わず、データ D が生じる確率

尤度と周辺尤度の関係を重視して、以下のように書く場合もある。

$$
P(H|D) = P(H) \frac{P(D|H)}{P(D)}
$$

P(H) は事前にデータ D が何もない状態で、仮説 H が正しい確率。
そこに仮説 H が正しいとした場合のデータ D の尤もらしさを掛けて、仮説 H の真偽を問わずデータ D を観察する可能性で割る。

さまざまなデータを取り込んで精度を向上させることをベイズ更新という。

$$
P(H|D) = \frac{P(H)P(D|H)}{P(H)P(D|H) + P(\overline{H})P(D|\overline{H})}
$$

https://bellcurve.jp/statistics/course/6444.html

https://www.headboost.jp/bayes-theorem/

https://qiita.com/ysk24ok/items/c87a73fd2ee16091ec91
